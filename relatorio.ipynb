{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Descritivos - Projeto Final\n",
    "### Residência de Ciência de Dados - CIN/UFPE\n",
    "\n",
    "- Lucas Couri (lncc2)\n",
    "- Mariama Oliveira (mcso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudo sobre geração de áudio utilizando VAE\n",
    "## 1) Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo do projeto é a geração de música a partir do uso de um Variational AutoEnconder, para tal partiu-se de um modelo utilizado para geração de fala.\n",
    "\n",
    "Portanto, o projeto seguiu duas etapas:\n",
    "\n",
    "1. Gerar áudio de dígitos aleatórios a partir do dataset de áudio de dígitos MNIST (https://www.kaggle.com/alanchn31/free-spoken-digits), utilizamos 3000 arquivos de áudio para o experimento.\n",
    "\n",
    "2. Verificar se a mesma metodologia pode ser aplicada para músicas, de forma a gerar composições musicais com base em músicas de determinado gênero musical disponíveis no dataset GTZAN (https://www.kaggle.com/andradaolteanu/gtzan-dataset-music-genre-classification).\n",
    "\n",
    "Ao final, deseja-se entender como VAE pode ser utilizado na geração de amostras de audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Metodologia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta seção serão descritos a base de dados e os passos e métodos utilizados para a realização do projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descrição das bases de dados\n",
    "\n",
    "**Base áudio de dígitos MNIST**\n",
    "\n",
    "Base que contém o resgistro em .wav dos dígitos de 0 a 9 de \n",
    "- 6 falantes\n",
    "- 3,000 áudios (50 por cada falante)\n",
    "- Pronúncia em inglês \n",
    "\n",
    "**Base áudio GTZAN**\n",
    "\n",
    "Base que contém o resgistro em .wav de músicas\n",
    "- 10 gêneros musicais (cada gênero 100 músicas)\n",
    "- 30 segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descrição do experimento\n",
    "\n",
    "Foram realizados os seguintes passos:\n",
    "\n",
    "1. Pré-processamento de dados\n",
    "\n",
    "2. Treinamento do modelo (VAE) -> O código do VAE se encontra no script autoencoder.py\n",
    "\n",
    "3. Geração da amostra a partir do espaço latente obtido no passo 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas e dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoencoder as ae\n",
    "import train as tr\n",
    "import preprocess as pp\n",
    "import soundgenerator as sg\n",
    "import generate as gn\n",
    "\n",
    "from soundgenerator import SoundGenerator\n",
    "from autoencoder import VAE\n",
    "import pickle\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento do dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O pipeline de pré-processamento consiste em 5 passos:\n",
    "\n",
    "1. Carregar arquivos .wav\n",
    "\n",
    "2. Aplicar padding caso seja necessário (quando a duração do arquivo é menor que a padrão)\n",
    "\n",
    "3. Extrair espectrogramas\n",
    "\n",
    "4. Normalizar os dados com Min Max\n",
    "\n",
    "5. Salvar os espectrogramas e os valores de MinMax para cada espectrograma.\n",
    "\n",
    "O código do pré-processamento está no script **preprocess.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executando pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido às características distintas da fala e música, a amostragem no carregamento de cada áudio é diferente, portanto abaixo são determinados os parâmetros para cada tipo de áudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametros de amostragem do audio\n",
    "FRAME_SIZE = 512\n",
    "HOP_LENGTH = 256\n",
    "DURATION = 0.74  # em segundos\n",
    "SAMPLE_RATE = 22050\n",
    "MONO = True\n",
    "\n",
    "#Path dos dados\n",
    "SPECTROGRAMS_PATH = \"data/fsdd/spectrograms/\"\n",
    "MIN_MAX_VALUES_SAVE_DIR = \"data/fsdd/\"\n",
    "FILES_DIR = \"data/recordings/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametros de amostragem de música (jazz)\n",
    "FRAME_SIZE = 1024\n",
    "HOP_LENGTH = 512\n",
    "DURATION = 5.94  # em segundos\n",
    "SAMPLE_RATE = 22050\n",
    "MONO = True\n",
    "\n",
    "#Path dos dados\n",
    "SPECTROGRAMS_PATH = \"data/fsdd-jazz/spectrograms/\"\n",
    "MIN_MAX_VALUES_SAVE_DIR = \"data/fsdd-jazz/\"\n",
    "FILES_DIR = \"data/jazz/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametros de amostragem de música (metal)\n",
    "FRAME_SIZE = 1024\n",
    "HOP_LENGTH = 512\n",
    "DURATION = 5.94  # em segundos\n",
    "SAMPLE_RATE = 22050\n",
    "MONO = True\n",
    "\n",
    "#Path dos dados\n",
    "SPECTROGRAMS_PATH = \"data/fsdd-metal/spectrograms/\"\n",
    "MIN_MAX_VALUES_SAVE_DIR = \"data/fsdd-metal/\"\n",
    "FILES_DIR = \"data/metal/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando objetos \n",
    "loader = pp.Loader(SAMPLE_RATE, DURATION, MONO)\n",
    "padder = pp.Padder()\n",
    "log_spectrogram_extractor = pp.LogSpectrogramExtractor(FRAME_SIZE, HOP_LENGTH)\n",
    "min_max_normaliser = pp.MinMaxNormaliser(0, 1)\n",
    "saver = pp.Saver(SPECTROGRAMS_PATH, MIN_MAX_VALUES_SAVE_DIR)\n",
    "\n",
    "preprocessing_pipeline = pp.PreprocessingPipeline()\n",
    "preprocessing_pipeline.loader = loader\n",
    "preprocessing_pipeline.padder = padder\n",
    "preprocessing_pipeline.extractor = log_spectrogram_extractor\n",
    "preprocessing_pipeline.normaliser = min_max_normaliser\n",
    "preprocessing_pipeline.saver = saver\n",
    "\n",
    "preprocessing_pipeline.process(FILES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitetura do Variational AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A arquitetura do Variational Autoencoder é composta por 5 camadas convolucionais, sendo cada uma composta por uma conv 2d, uma camada ReLU e uma camada de batch normalization. No meio do encoder/decoder há também uma camada densa onde é feito o cálculo do espaço latente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo foi treinado a partir dos espectogramas extraídos anteriormente, eles tem tamanho 256x64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros para áudios de dígitos\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 150\n",
    "DIR_MODEL = \"model/digits/\"\n",
    "#SPECTROGRAMS_PATH = \"data/fsdd/spectrograms/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros para músicas (jazz)\n",
    "LEARNING_RATE =  0.0005\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 6\n",
    "DIR_MODEL = \"model/jazz/\"\n",
    "#SPECTROGRAMS_PATH = \"data/fsdd//\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros para músicas (metal)\n",
    "LEARNING_RATE =  0.0005\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 6\n",
    "DIR_MODEL = \"model/metal/\"\n",
    "#SPECTROGRAMS_PATH = \"data/fsdd//\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tr.load_fsdd(SPECTROGRAMS_PATH)\n",
    "autoencoder = tr.train(x_train, LEARNING_RATE, BATCH_SIZE, EPOCHS)\n",
    "autoencoder.save(DIR_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruindo amostra a partir Espaço Latente\n",
    "\n",
    "- Falar da abordagem de pegar uma musica e mapear no espaço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretórios dos áudios dos dígitos\n",
    "SAVE_DIR_ORIGINAL = \"samples/digits/original/\"\n",
    "SAVE_DIR_GENERATED = \"samples/digits/generated/\"\n",
    "MIN_MAX_VALUES_PATH = \"data/fsdd/min_max_values.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretórios das músicas (jazz)\n",
    "SAVE_DIR_ORIGINAL = \"samples/jazz/original/\"\n",
    "SAVE_DIR_GENERATED = \"samples/jazz/generated/\"\n",
    "MIN_MAX_VALUES_PATH = \"data/fsdd-jazz/min_max_values.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretórios das músicas (metal)\n",
    "SAVE_DIR_ORIGINAL = \"samples/metal/original/\"\n",
    "SAVE_DIR_GENERATED = \"samples/metal/generated/\"\n",
    "MIN_MAX_VALUES_PATH = \"data/fsdd-metal/min_max_values.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Carregar modelo treinado\n",
    "vae = VAE.load(DIR_MODEL)\n",
    "sound_generator = SoundGenerator(vae, HOP_LENGTH)\n",
    "\n",
    "# Carregar espectrograma + valores min max\n",
    "with open(MIN_MAX_VALUES_PATH, \"rb\") as f:\n",
    "    min_max_values = pickle.load(f)\n",
    "\n",
    "specs, file_paths = gn.load_fsdd(SPECTROGRAMS_PATH)\n",
    "\n",
    "# sample espectrograma + valores min max\n",
    "sampled_specs, sampled_min_max_values = gn.select_spectrograms(specs,\n",
    "                                                            file_paths,\n",
    "                                                            min_max_values,\n",
    "                                                            5)\n",
    "\n",
    "# Gerar audio a partir do espaço latente\n",
    "signals, _ = sound_generator.generate(sampled_specs,\n",
    "                                        sampled_min_max_values)\n",
    "\n",
    "# Convert espectrograma para arquivo de audio\n",
    "original_signals = sound_generator.convert_spectrograms_to_audio(\n",
    "    sampled_specs, sampled_min_max_values)\n",
    "\n",
    "# Salva sinais de audio\n",
    "gn.save_signals(signals, SAVE_DIR_GENERATED)\n",
    "gn.save_signals(original_signals, SAVE_DIR_ORIGINAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Análise de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para executar os áudios é necessário rodar o notebook utilizando o Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados dos áudios de dígitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Audio original 1\n",
    "audio1_orig = SAVE_DIR_ORIGINAL+\"0.wav\"\n",
    "ipd.Audio(audio1_orig, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Audio gerado 1\n",
    "audio1_gen = SAVE_DIR_GENERATED+\"0.wav\"\n",
    "ipd.Audio(audio1_gen, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Audio original 2\n",
    "audio2_orig = SAVE_DIR_ORIGINAL+\"3.wav\"\n",
    "ipd.Audio(audio2_orig, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Audio gerado 2\n",
    "audio2_gen = SAVE_DIR_GENERATED+\"3.wav\"\n",
    "ipd.Audio(audio2_gen, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados das músicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jazz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Musica original 1\n",
    "audio1_orig = \"samples/jazz/original/0.wav\"\n",
    "ipd.Audio(audio1_orig, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Musica gerado 1\n",
    "audio1_gen = \"samples/jazz/generated/0.wav\"\n",
    "ipd.Audio(audio1_gen, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Musica original 2\n",
    "audio2_orig = \"samples/jazz/original/3.wav\"\n",
    "ipd.Audio(audio2_orig, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Musica gerado 2\n",
    "audio2_gen = \"samples/jazz/generated/3.wav\"\n",
    "ipd.Audio(audio2_gen, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Musica original 1\n",
    "audio1_orig = \"samples/metal/original/0.wav\"\n",
    "ipd.Audio(audio1_orig, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Musica gerado 1\n",
    "audio1_gen = \"samples/metal/generated/0.wav\"\n",
    "ipd.Audio(audio1_gen, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Musica original 2\n",
    "audio2_orig = \"samples/metal/original/3.wav\"\n",
    "ipd.Audio(audio2_orig, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Musica gerado 2\n",
    "audio2_gen = \"samples/metal/generated/3.wav\"\n",
    "ipd.Audio(audio2_gen, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Considerações finais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentários gerais:\n",
    "\n",
    "- A arquitetura funcionou bem para o áudio dos dígitos, conseguindo reproduzir com poucas perdas o mesmo dígito. Já para as músicas notamos que não gerou áudios similares aos originais, isso pode ser devido a vários motivos. \n",
    "\n",
    "- Um palpite seria que talvez o uso de uma arquitetura recorrente forneça melhores resultados para este tipo de problema, uma vez que vários trabalhos e artigos da área utilizam redes recorrentes como solução.\n",
    "\n",
    "- Outra possibilidade é a amostragem do pré-processamento, que tem uma quantidade limitada de amostras, já que música necessita de mais dados do que fala para representar informações. \n",
    "\n",
    "- Talvez seja melhor representar as músicas em outro tipo de codificação, mais compacta, como a midi. Isso pode fornecer uma melhor informação sem perda de qualidade. \n",
    "\n",
    "- Outra abordagem interessante é investir em técnicas de decomposição da forma de onda para extrair as features mais importantes da música. \n",
    "\n",
    "- Vale notar que o dataset dos áudios dos dígitos é bem mais extenso do que o dataset das músicas, além da complexidade de informações dos dois casos ser bastante discrepante.\n",
    "\n",
    "- Tivemos dificuldade na etapa de treinamento do modelo, devido ao custo computacional dessa tarefa ocasionado pela grande dimensionalidade dos dados. Uma possível abordagem é utilizar técnicas para reduzir previamente a dimensionalidade dos dados.\n",
    "\n",
    "Vimos que o Convolutional Variational Autoencoder é bastante utilizado para geração de composições musicais, no entanto as arquiteturas que são geralmente utilizadas são mais complexas do que as utilizadas neste projeto. Mas apesar dos resultados ruins para músicas o processo auxiliou no entendimento da arquitetura e seu funcionamento."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8003219c8c57211ee3be347d121ba14ebad7276cdae3d94be72d9e4e17f9edd5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
